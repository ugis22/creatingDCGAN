{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries requiered\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.layers import (\n",
    "    conv2d_transpose,\n",
    "    conv2d\n",
    ")\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.nn import (\n",
    "    relu,\n",
    "    tanh,\n",
    "    sigmoid,\n",
    "    leaky_relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(input_data, initial_dim, training, reuse=False):\n",
    "    \"\"\"\n",
    "    The function takes a vector and generates an images through a deconvolution neural net.\n",
    "    The parameters that need to be passed are: \n",
    "    * input_data: a vector \n",
    "    * initial_dim: the shape of the input data\n",
    "    * Training: False or True, corresponding to whether the net is still on training or not\n",
    "    * Reuse: Set to default to False, If True the function will reuse the variables already created\n",
    "    \"\"\"\n",
    "    #Specifing the size of the different layers\n",
    "    layer_size_1 = 512\n",
    "    layer_size_2 = 256\n",
    "    layer_size_3 = 128\n",
    "    layer_size_4 = 64\n",
    "    #layer_size_5 = 32\n",
    "    \n",
    "    #Final dimension output should be 3, because RGB images are wanted (one for each channel)\n",
    "    final_layer = 3\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        #Project and reshape layer that will map the noise vector of shape 100x1 to a layer of 4*4*512\n",
    "        #First a variable for the initial weights should be created\n",
    "        #Shape should be 4*4*512\n",
    "        #Random dimension is the dimension of the noise vector: 100\n",
    "        #Standard deviation is selected according to literature\n",
    "        weights_1 = tf.variable('weights_1', shape=[initial_dimension, 4*4*512], dtype=tf.float32, initizialiter=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        #Create a variable for the initial bias\n",
    "        bias_1 = tf.variable('bias_1', shape=[layer_size_1*4*4], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "        #Add the first layer\n",
    "        act_layer_1 = tf.add(tf.matmul(input_data, weights_1), bias_1, name='act_layer_1')\n",
    "        \n",
    "\n",
    "        #Each layer should contain three steps:\n",
    "        #Deconvolutional layer2\n",
    "        #convolution\n",
    "        conv_layer_2 = conv2d_transpose(act_layer_1, layer_size_2, kernel_size=[4, 4], strides=[2, 2], padding='SAME', kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv_layer_2')\n",
    "        #bias\n",
    "        bias_transf_2 = batch_norm(conv_layer_2, decay=0.9, epsilon=1e-5, is_training=training, scope='bias_transf_2')    \n",
    "        #activation\n",
    "        act_layer_2 = relu(bias_transf_2, name='act_layer_2')\n",
    "\n",
    "        #Deconvolutional layer3\n",
    "        #convolution\n",
    "        conv_layer_3 = conv2d_transpose(act_layer_2, layer_size_3, kernel_size=[4, 4], strides=[2, 2], padding='SAME', kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv_layer_3')\n",
    "        #bias\n",
    "        bias_transf_3 = batch_norm(conv_layer_3, decay=0.9, epsilon=1e-5, is_training=training, scope='bias_transf_3')    \n",
    "        #activation\n",
    "        act_layer_3 = relu(bias_transf_3, name='act_layer_3')\n",
    "\n",
    "        #Deconvolutional layer4\n",
    "        #convolution\n",
    "        conv_layer_4 = conv2d_transpose(act_layer_3, layer_size_4, kernel_size=[4, 4], strides=[2, 2], padding='SAME', kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv_layer_4')\n",
    "        #bias\n",
    "        bias_transf_4 = batch_norm(conv_layer_4, decay=0.9, epsilon=1e-5, is_training=training, scope='bias_transf_4')    \n",
    "        #activation\n",
    "        act_layer_4 = relu(bias_transf_4, name='act_layer_4')\n",
    "\n",
    "        #Deconvolutional layer5\n",
    "        #convolution\n",
    "        #conv_layer_5 = tf.layers.conv2d_transpose(act_layer_4, layer_size_5, kernel_size=[4, 4], strides=[2, 2], padding='SAME', kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv_layer_5')\n",
    "        #bias\n",
    "        #bias_transf_5 = tf.contrib.layers.batch_norm(conv_layer_5, decay=0.9, epsilon=1e-5, is_training=, scope='bias_transf_5')    \n",
    "        #activation\n",
    "        #act_layer_5 = tf.nn.relu(bias_transf_5, name='act_layer_5')\n",
    "\n",
    "        #Final deconvolutional layer squash everything together to get a RGB image\n",
    "        final_conv = conv2d_transpose(act_layer_4, final_layer, kernel_size=[4, 4], strides=[2, 2], padding='SAME', kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='final_conv')\n",
    "        #For the last layer always should be used tahn act function\n",
    "        final_act = tahn(final_conv, name='final_act')\n",
    "        return final_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
